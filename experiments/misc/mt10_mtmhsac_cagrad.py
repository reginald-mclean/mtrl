from dataclasses import dataclass
from pathlib import Path

import tyro

from mtrl.config.networks import ContinuousActionPolicyConfig, QValueFunctionConfig
from mtrl.config.nn import MultiHeadConfig, NeuralNetworkConfig, VanillaNetworkConfig
from mtrl.config.optim import OptimizerConfig, CAGradConfig
from mtrl.config.rl import OffPolicyTrainingConfig
from mtrl.envs import MetaworldConfig
from mtrl.experiment import Experiment
from mtrl.rl.algorithms import MTSACConfig



@dataclass(frozen=True)
class Args:
    seed: int = 1
    track: bool = False
    wandb_project: str | None = None
    wandb_entity: str | None = None
    data_dir: Path = Path("./experiment_results")
    resume: bool = False
    clip: bool = False
    width: int = 400
    reward_func_version: str = "v2"


def main() -> None:
    args = tyro.cli(Args)

    experiment = Experiment(
        exp_name=f"mt10_mtmhsac_CAGrad_clip_{args.width}_rf_{args.reward_func_version}" if args.clip else f"mt10_mtmhsac_CAGrad_no_clip_{args.width}_rf_{args.reward_func_version}",
        seed=args.seed,
        data_dir=args.data_dir,
        env=MetaworldConfig(
            env_id="MT10",
            reward_func_version=args.reward_func_version,
            terminate_on_success=False,
        ),
        algorithm=MTSACConfig(
            num_tasks=10,
            gamma=0.99,
            clip=args.clip,
            actor_config=ContinuousActionPolicyConfig(
                network_config=VanillaNetworkConfig(
                    #num_tasks=10,
                    width=args.width,
                    optimizer=CAGradConfig(
                        num_tasks=10,
                        max_grad_norm=1.0,
                        cagrad_optimizer=OptimizerConfig(max_grad_norm=1.0),
                    ),
                )
            ),
            critic_config=QValueFunctionConfig(
                network_config=VanillaNetworkConfig(
                    #num_tasks=10,
                    width=args.width,
                    optimizer=CAGradConfig(
                        num_tasks=10,
                        max_grad_norm=1.0,
                        cagrad_optimizer=OptimizerConfig(max_grad_norm=1.0),
                    ),
                )
            ),
            num_critics=2,
        ),
        training_config=OffPolicyTrainingConfig(
            total_steps=int(2e7),
            buffer_size=int(1e6),
            batch_size=1280,
        ),
        checkpoint=True,
        resume=args.resume,
    )

    if args.track:
        assert args.wandb_project is not None and args.wandb_entity is not None
        experiment.enable_wandb(
            project=args.wandb_project,
            entity=args.wandb_entity,
            config=experiment,
            resume="allow",
        )

    experiment.run()


if __name__ == "__main__":
    main()
